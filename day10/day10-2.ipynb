{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb811ce-885e-4c25-bb18-9769087eb055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------迁移学习-----------------------------\n",
    "#预训练模型和微调\n",
    "#预训练模型：BERT GPT\n",
    "#两种迁移方式：直接使用预训练模型 、微调（再特定领域进行垂直调整）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40d7e478-21cb-4473-b459-2fe6801ba704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer库的使用\n",
    "#管道pipline(代码简单) 自动Automode 和具体模型方式specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cf4c77b-1c51-4b93-bec6-1b490efca267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ec32086-16be-4eb1-bb0f-662bc9f04ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result--》[{'label': 'star 5', 'score': 0.250393807888031}]\n"
     ]
    }
   ],
   "source": [
    "#文本分类的任务\n",
    "def dm01_test_classcify():\n",
    "    # 基于pipeline函数返回需要的模型\n",
    "    # model = pipeline(task=\"sentiment-analysis\", model='./model/chinese_sentiment')#文本分类任务\n",
    "    model = pipeline(task=\"text-classification\", model=r'C:\\Users\\Lenovo\\Desktop\\chinese_sentiment',device = device)\n",
    "    # 直接使用model来预测\n",
    "    result = model('我想见你')\n",
    "    print(f'result--》{result}')\n",
    "dm01_test_classcify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ee11861-2786-4a6c-a9dd-97b545a856fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17, 768])\n"
     ]
    }
   ],
   "source": [
    "#特征提取任务\n",
    "#文本分类的任务\n",
    "def dm02_test_classcify():\n",
    "    # 基于pipeline函数返回需要的模型\n",
    "    # model = pipeline(task=\"sentiment-analysis\", model='./model/chinese_sentiment')#文本分类任务\n",
    "    model = pipeline(task=\"feature-extraction\", model=r'D:\\bert',device = device)\n",
    "    # 直接使用model来预测\n",
    "    result = model('我爱北京天安门，天安门上太阳升')\n",
    "    print(torch.tensor(result).size())\n",
    "dm02_test_classcify()#将单词变成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c550561-4296-4ed6-b185-26f173a3a62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:\\bert-wwm were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.35923588275909424, 'token': 872, 'token_str': '你', 'sequence': '我 想 明 天 去 你 家 吃 饭'}, {'score': 0.25940313935279846, 'token': 1961, 'token_str': '她', 'sequence': '我 想 明 天 去 她 家 吃 饭'}, {'score': 0.22949299216270447, 'token': 2769, 'token_str': '我', 'sequence': '我 想 明 天 去 我 家 吃 饭'}, {'score': 0.08896258473396301, 'token': 800, 'token_str': '他', 'sequence': '我 想 明 天 去 他 家 吃 饭'}, {'score': 0.01073597650974989, 'token': 2644, 'token_str': '您', 'sequence': '我 想 明 天 去 您 家 吃 饭'}]\n"
     ]
    }
   ],
   "source": [
    "#完形填空任务\n",
    "def dm03_test_classcify():\n",
    "    # 基于pipeline函数返回需要的模型\n",
    "    # model = pipeline(task=\"sentiment-analysis\", model='./model/chinese_sentiment')#文本分类任务\n",
    "    model = pipeline(task=\"fill-mask\", model=r'D:\\bert-wwm',device = device)\n",
    "    # 直接使用model来预测\n",
    "    result = model('我想明天去[MASK]家吃饭')\n",
    "    print(result)\n",
    "dm03_test_classcify()#将单词变成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86b32844-d4fe-4f92-b2ca-0592f457f0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:\\roberta were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 1.207176177596414e-12, 'start': 2, 'end': 4, 'answer': '张三'}, {'score': 2.6089412585861282e-06, 'start': 9, 'end': 12, 'answer': '程序员'}, {'score': 4.1686924134864967e-08, 'start': 18, 'end': 21, 'answer': '打篮球'}]\n"
     ]
    }
   ],
   "source": [
    "#阅读理解\n",
    "def dm04_test_classcify():\n",
    "    # 基于pipeline函数返回需要的模型\n",
    "    context = \"我叫张三，我是一个程序员，我的喜好是打篮球。\"\n",
    "    questions = ['我是谁？', '我是做什么的？', '我的爱好是什么？']\n",
    "    # model = pipeline(task=\"sentiment-analysis\", model='./model/chinese_sentiment')#文本分类任务\n",
    "    model = pipeline(task=\"question-answering\", model=r'D:\\roberta',device = device)\n",
    "    # 直接使用model来预测\n",
    "    result = model(context = context,question = questions)\n",
    "    print(result)\n",
    "dm04_test_classcify()#将单词变成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4dbef70-e15d-48f3-89c5-5b6897292c55",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "D:\\cnn does not appear to have a file named config.json. Checkout 'https://huggingface.co/D:\\cnn/tree/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     result \u001b[38;5;241m=\u001b[39m model(text)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult--》\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mdm05_test_classcify\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#将单词变成词向量\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[52], line 8\u001b[0m, in \u001b[0;36mdm05_test_classcify\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m questions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m我是谁？\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m我是做什么的？\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m我的爱好是什么？\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# model = pipeline(task=\"sentiment-analysis\", model='./model/chinese_sentiment')#文本分类任务\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcnn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 直接使用model来预测\u001b[39;00m\n\u001b[0;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERT is a transformers model pretrained on a large corpus of English data \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m     11\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a self-supervised fashion. This means it was pretrained on the raw texts \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m     12\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly, with no humans labelling them in any way (which is why it can use lots \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences that were next to each other in the original text, sometimes not. The model then \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m     23\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas to predict if the two sentences were following each other or not.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DL\\lib\\site-packages\\transformers\\pipelines\\__init__.py:805\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    802\u001b[0m                 adapter_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    803\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 805\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    806\u001b[0m         model, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, code_revision\u001b[38;5;241m=\u001b[39mcode_revision, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[0;32m    807\u001b[0m     )\n\u001b[0;32m    808\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m    810\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\.conda\\envs\\DL\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:976\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    974\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 976\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    977\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    978\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32m~\\.conda\\envs\\DL\\lib\\site-packages\\transformers\\configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\DL\\lib\\site-packages\\transformers\\configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DL\\lib\\site-packages\\transformers\\utils\\hub.py:373\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[1;32m--> 373\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    374\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    375\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    376\u001b[0m         )\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: D:\\cnn does not appear to have a file named config.json. Checkout 'https://huggingface.co/D:\\cnn/tree/None' for available files."
     ]
    }
   ],
   "source": [
    "#文本摘要\n",
    "#阅读理解\n",
    "def dm05_test_classcify():\n",
    "    # 基于pipeline函数返回需要的模型\n",
    "    context = \"我叫张三，我是一个程序员，我的喜好是打篮球。\"\n",
    "    questions = ['我是谁？', '我是做什么的？', '我的爱好是什么？']\n",
    "    # model = pipeline(task=\"sentiment-analysis\", model='./model/chinese_sentiment')#文本分类任务\n",
    "    model = pipeline(task=\"summarization\", model=r'D:\\cnn',device = device)\n",
    "    # 直接使用model来预测\n",
    "    text = \"BERT is a transformers model pretrained on a large corpus of English data \" \\\n",
    "           \"in a self-supervised fashion. This means it was pretrained on the raw texts \" \\\n",
    "           \"only, with no humans labelling them in any way (which is why it can use lots \" \\\n",
    "           \"of publicly available data) with an automatic process to generate inputs and \" \\\n",
    "           \"labels from those texts. More precisely, it was pretrained with two objectives:Masked \" \\\n",
    "           \"language modeling (MLM): taking a sentence, the model randomly masks 15% of the \" \\\n",
    "           \"words in the input then run the entire masked sentence through the model and has \" \\\n",
    "           \"to predict the masked words. This is different from traditional recurrent neural \" \\\n",
    "           \"networks (RNNs) that usually see the words one after the other, or from autoregressive \" \\\n",
    "           \"models like GPT which internally mask the future tokens. It allows the model to learn \" \\\n",
    "           \"a bidirectional representation of the sentence.Next sentence prediction (NSP): the models\" \\\n",
    "           \" concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to \" \\\n",
    "           \"sentences that were next to each other in the original text, sometimes not. The model then \" \\\n",
    "           \"has to predict if the two sentences were following each other or not.\"\n",
    "    result = model(text)\n",
    "    print(f'result--》{result}')\n",
    "dm05_test_classcify()#将单词变成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4937d0a-0acd-4bd2-a5d2-595da7c6aca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'address', 'score': 0.7181479, 'word': '北 京 天 安 门', 'start': 2, 'end': 7}, {'entity_group': 'address', 'score': 0.56800413, 'word': '天 安 门', 'start': 8, 'end': 11}]\n"
     ]
    }
   ],
   "source": [
    "#NER识别出专有名词\n",
    "def dm06_ner():\n",
    "\n",
    "    # 基于pipeline函数返回需要的模型\n",
    "    model = pipeline(task=\"ner\", model=r'D:\\cnn',device = device,aggregation_strategy=\"simple\")\n",
    "    # 直接使用model来预测\n",
    "    result = model(\"我爱北京天安门，天安门上太阳升\")\n",
    "    # print(f'result--》{result}')\n",
    "    print(result)\n",
    "dm06_ner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88ddc4-6d3b-4a55-ae92-81b98a80c89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

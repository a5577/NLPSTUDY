{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686b672b-eba8-48f8-aae5-bb6e5fafd65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN代码实现\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169de740-63d3-4f3d-9cd1-178e39da5895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4475e-01,  5.1469e-01,  4.0114e-01,  6.4837e-01,  7.7629e-04,\n",
      "          -3.3182e-01],\n",
      "         [ 3.6517e-01, -9.5777e-01,  9.1682e-01,  3.7413e-01,  2.6056e-01,\n",
      "          -8.6917e-01],\n",
      "         [-5.1309e-01,  5.6946e-01,  6.6815e-01, -3.0154e-01,  7.4923e-01,\n",
      "           5.6983e-01]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[ 1.4475e-01,  5.1469e-01,  4.0114e-01,  6.4837e-01,  7.7629e-04,\n",
      "          -3.3182e-01],\n",
      "         [ 3.6517e-01, -9.5777e-01,  9.1682e-01,  3.7413e-01,  2.6056e-01,\n",
      "          -8.6917e-01],\n",
      "         [-5.1309e-01,  5.6946e-01,  6.6815e-01, -3.0154e-01,  7.4923e-01,\n",
      "           5.6983e-01]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def _dm_rnn_for_base():\n",
    "#实例化模型\n",
    "    #RNN参数说明 ： 第一个参数：输入的词嵌入维度input_size  第二个参数 hidden_size RNN单元输出的隐藏层张量的维度 \n",
    "    #第三个参数num_layer  有几层RNN单元  有几个隐藏层\n",
    "    rnn = nn.RNN(5,6,1)\n",
    "    #获取x输入\n",
    "    #第一个参数：sequence_length输入序列的长度(一个句子词汇或者字符的个数)\n",
    "    #第二个参数：batch_size:批次样本数量  一个批次送入几个样本\n",
    "    #第三个参数：input_size:输入张量x的维度\n",
    "    x0 = torch.randn(1,3,5)\n",
    "    #获取h0\n",
    "    #第一个参数num_player\n",
    "    #第二个参数 batch_size\n",
    "    #第三个参数 hidden_size \n",
    "    h0 = torch.randn(1,3,6)\n",
    "    #输出结果\n",
    "    output,hn = rnn(x0,h0)\n",
    "    print(output)\n",
    "    print(hn)\n",
    "_dm_rnn_for_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c324054-ece0-4175-ad8a-f94cd2c261b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#句子长度改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595f24cb-6bfd-4295-90d0-0268c2431d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-4.6621e-01, -2.4433e-01,  2.2742e-01, -6.4198e-01,  4.9413e-01,\n",
      "          -5.5326e-01],\n",
      "         [ 3.0374e-01, -8.1310e-01, -8.6633e-01,  3.5637e-01, -7.4608e-01,\n",
      "           6.9199e-02],\n",
      "         [-5.2572e-01,  2.7557e-01,  2.8967e-01,  8.1470e-01,  5.0812e-01,\n",
      "          -7.8237e-01]],\n",
      "\n",
      "        [[ 5.0606e-01, -2.7891e-01, -7.7449e-01,  3.4649e-01,  2.0488e-01,\n",
      "          -1.3732e-01],\n",
      "         [ 2.0413e-01,  5.0399e-01,  2.1051e-01,  1.8257e-02, -4.9122e-01,\n",
      "          -7.1567e-01],\n",
      "         [-3.2222e-01, -3.8835e-01, -7.9815e-01,  9.5559e-02, -8.2489e-01,\n",
      "           5.9557e-04]],\n",
      "\n",
      "        [[-2.2126e-01, -8.0546e-02, -5.1195e-01, -1.4237e-01, -8.4744e-01,\n",
      "          -2.3261e-01],\n",
      "         [-6.8840e-01,  2.7741e-02,  2.0167e-01, -1.3627e-01, -7.2440e-01,\n",
      "          -4.9092e-01],\n",
      "         [-1.6534e-01,  2.1826e-01,  6.0202e-02, -4.1312e-01, -5.1967e-01,\n",
      "          -3.0298e-01]],\n",
      "\n",
      "        [[-1.6417e-01,  2.7976e-01, -4.6899e-01, -7.6135e-01, -8.9985e-01,\n",
      "          -2.0597e-02],\n",
      "         [-1.8113e-01, -3.6350e-01, -4.1662e-01, -4.6539e-01, -4.7487e-01,\n",
      "          -1.5889e-01],\n",
      "         [-3.1027e-01,  3.0756e-01, -4.3223e-01, -3.8272e-01, -6.4289e-01,\n",
      "          -4.1160e-01]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[-0.1642,  0.2798, -0.4690, -0.7614, -0.8999, -0.0206],\n",
      "         [-0.1811, -0.3635, -0.4166, -0.4654, -0.4749, -0.1589],\n",
      "         [-0.3103,  0.3076, -0.4322, -0.3827, -0.6429, -0.4116]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def _dm_rnn_for_base():\n",
    "#实例化模型\n",
    "    #RNN参数说明 ： 第一个参数：输入的词嵌入维度input_size  第二个参数 hidden_size RNN单元输出的隐藏层张量的维度 \n",
    "    #第三个参数num_layer  有几层RNN单元  有几个隐藏层\n",
    "    rnn = nn.RNN(5,6,1)\n",
    "    #获取x输入\n",
    "    #第一个参数：sequence_length输入序列的长度(一个句子词汇或者字符的个数)\n",
    "    #第二个参数：batch_size:批次样本数量  一个批次送入几个样本\n",
    "    #第三个参数：input_size:输入张量x的维度\n",
    "    x0 = torch.randn(4,3,5)\n",
    "    #获取h0\n",
    "    #第一个参数num_player\n",
    "    #第二个参数 batch_size\n",
    "    #第三个参数 hidden_size \n",
    "    h0 = torch.randn(1,3,6)\n",
    "    #输出结果\n",
    "    output,hn = rnn(x0,h0)\n",
    "    print(output)\n",
    "    print(hn)\n",
    "_dm_rnn_for_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8963251f-ed5a-46ce-aa68-bbec50a991a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7897, -0.7483, -0.8416, -0.7269,  0.3797, -0.5493]],\n",
      "\n",
      "        [[ 0.4613, -0.1187,  0.2253, -0.7787, -0.1960, -0.2920]],\n",
      "\n",
      "        [[ 0.6021, -0.5757, -0.7103, -0.2461, -0.7222,  0.5795]],\n",
      "\n",
      "        [[ 0.6594,  0.0766, -0.7644,  0.0168,  0.1258,  0.5694]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.6594,  0.0766, -0.7644,  0.0168,  0.1258,  0.5694]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.7897, -0.7483, -0.8416, -0.7269,  0.3797, -0.5493]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.7897, -0.7483, -0.8416, -0.7269,  0.3797, -0.5493]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.4613, -0.1187,  0.2253, -0.7787, -0.1960, -0.2920]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.4613, -0.1187,  0.2253, -0.7787, -0.1960, -0.2920]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.6021, -0.5757, -0.7103, -0.2461, -0.7222,  0.5795]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.6021, -0.5757, -0.7103, -0.2461, -0.7222,  0.5795]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.6594,  0.0766, -0.7644,  0.0168,  0.1258,  0.5694]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.6594,  0.0766, -0.7644,  0.0168,  0.1258,  0.5694]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def _dm_rnn_for_base():\n",
    "#实例化模型\n",
    "    #RNN参数说明 ： 第一个参数：输入的词嵌入维度input_size  第二个参数 hidden_size RNN单元输出的隐藏层张量的维度 \n",
    "    #第三个参数num_layer  有几层RNN单元  有几个隐藏层\n",
    "    rnn = nn.RNN(5,6,1)\n",
    "    #获取x输入\n",
    "    #第一个参数：sequence_length输入序列的长度(一个句子词汇或者字符的个数)\n",
    "    #第二个参数：batch_size:批次样本数量  一个批次送入几个样本\n",
    "    #第三个参数：input_size:输入张量x的维度\n",
    "    x0 = torch.randn(4,1,5)\n",
    "    #获取h0\n",
    "    #第一个参数num_player\n",
    "    #第二个参数 batch_size\n",
    "    #第三个参数 hidden_size \n",
    "    h0 = torch.randn(1,1,6)\n",
    "    #输出结果\n",
    "    output,hn = rnn(x0,h0)\n",
    "    print(output)\n",
    "    print(hn)\n",
    "    #将一个token 一个token往RNN模型里面送\n",
    "    for idx in range(x0.shape[0]):#代表sequence_length\n",
    "        temp = x0[idx].unsqueeze(dim = 0)#二维变三维\n",
    "        output,h0 =rnn(temp,h0)#循环\n",
    "        print(output)\n",
    "        print(h0)\n",
    "_dm_rnn_for_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a8acc77-2183-4c35-94a8-93d00b2b01d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4235,  0.4596, -0.6197, -0.6307, -0.8504, -0.5648],\n",
      "         [ 0.8918,  0.6452,  0.3745, -0.4796, -0.2775,  0.8483],\n",
      "         [ 0.5849,  0.7334, -0.6659,  0.1901,  0.2166,  0.4629]],\n",
      "\n",
      "        [[ 0.4173, -0.4564, -0.7676,  0.1303,  0.2234,  0.5136],\n",
      "         [ 0.0802, -0.5965,  0.3619,  0.0055, -0.2661, -0.0193],\n",
      "         [ 0.6077,  0.6183, -0.4733, -0.7576, -0.0607, -0.6488]],\n",
      "\n",
      "        [[ 0.2232,  0.0497,  0.4646,  0.0385,  0.7089, -0.2172],\n",
      "         [-0.2367, -0.2127, -0.9294,  0.0640, -0.1368,  0.6271],\n",
      "         [ 0.7123,  0.9003,  0.7946, -0.8722, -0.6208, -0.8727]],\n",
      "\n",
      "        [[ 0.0924,  0.5402, -0.7163, -0.6946,  0.0687,  0.0461],\n",
      "         [ 0.3509, -0.5706, -0.9082,  0.0647, -0.2172,  0.6913],\n",
      "         [-0.0021, -0.7792, -0.9791,  0.2802,  0.1854,  0.3680]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.0924,  0.5402, -0.7163, -0.6946,  0.0687,  0.0461],\n",
      "         [ 0.3509, -0.5706, -0.9082,  0.0647, -0.2172,  0.6913],\n",
      "         [-0.0021, -0.7792, -0.9791,  0.2802,  0.1854,  0.3680]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def _dm_rnn_for_base():\n",
    "#实例化模型\n",
    "    #RNN参数说明 ： 第一个参数：输入的词嵌入维度input_size  第二个参数 hidden_size RNN单元输出的隐藏层张量的维度 \n",
    "    #第三个参数num_layer  有几层RNN单元  有几个隐藏层\n",
    "    rnn = nn.RNN(5,6,1)\n",
    "    #获取x输入\n",
    "    #第一个参数：sequence_length输入序列的长度(一个句子词汇或者字符的个数)\n",
    "    #第二个参数：batch_size:批次样本数量  一个批次送入几个样本\n",
    "    #第三个参数：input_size:输入张量x的维度\n",
    "    x0 = torch.randn(4,3,5)\n",
    "    #获取h0\n",
    "    #第一个参数num_player\n",
    "    #第二个参数 batch_size\n",
    "    #第三个参数 hidden_size \n",
    "    h0 = torch.randn(1,3,6)\n",
    "    #输出结果\n",
    "    output,hn = rnn(x0,h0)\n",
    "    print(output)\n",
    "    print(hn)\n",
    "_dm_rnn_for_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05cc3809-820f-42fa-b23e-1e0c10f457f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1869,  0.1396, -0.7501,  0.6648,  0.9398, -0.6823],\n",
      "         [ 0.7430,  0.6476, -0.5674, -0.2931,  0.4766, -0.8404],\n",
      "         [ 0.9032, -0.4636,  0.0863,  0.3694,  0.8282, -0.2320]],\n",
      "\n",
      "        [[ 0.6504,  0.4007, -0.1135, -0.0860,  0.2323,  0.2683],\n",
      "         [ 0.2186,  0.4837, -0.6659,  0.4030,  0.8617,  0.1602],\n",
      "         [ 0.5656,  0.4557, -0.5838, -0.2525,  0.5348,  0.3986]],\n",
      "\n",
      "        [[ 0.7294,  0.1821, -0.3088, -0.2745,  0.3704, -0.0977],\n",
      "         [ 0.7142,  0.4529,  0.0542, -0.0086, -0.1970,  0.2093],\n",
      "         [ 0.6910,  0.2713,  0.0412, -0.0109,  0.1939,  0.2245]],\n",
      "\n",
      "        [[ 0.6047,  0.3017, -0.4820,  0.0113,  0.7262,  0.0080],\n",
      "         [ 0.2757,  0.6819, -0.8499,  0.1067,  0.7410, -0.1516],\n",
      "         [ 0.7363,  0.5097, -0.6560, -0.2175,  0.3469,  0.0630]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.1110,  0.2467,  0.3876, -0.9499,  0.4941,  0.4571],\n",
      "         [ 0.3994, -0.7440,  0.4954, -0.8042,  0.0825,  0.0927],\n",
      "         [-0.4516,  0.2247,  0.9018, -0.6542, -0.6845,  0.7498]],\n",
      "\n",
      "        [[ 0.6047,  0.3017, -0.4820,  0.0113,  0.7262,  0.0080],\n",
      "         [ 0.2757,  0.6819, -0.8499,  0.1067,  0.7410, -0.1516],\n",
      "         [ 0.7363,  0.5097, -0.6560, -0.2175,  0.3469,  0.0630]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#多个RNN单元\n",
    "def _dm_rnn_for_base():\n",
    "#实例化模型\n",
    "    #RNN参数说明 ： 第一个参数：输入的词嵌入维度input_size  第二个参数 hidden_size RNN单元输出的隐藏层张量的维度 \n",
    "    #第三个参数num_layer  有几层RNN单元  有几个隐藏层\n",
    "    rnn = nn.RNN(5,6,2)\n",
    "    #获取x输入\n",
    "    #第一个参数：sequence_length输入序列的长度(一个句子词汇或者字符的个数)\n",
    "    #第二个参数：batch_size:批次样本数量  一个批次送入几个样本\n",
    "    #第三个参数：input_size:输入张量x的维度\n",
    "    x0 = torch.randn(4,3,5)\n",
    "    #获取h0\n",
    "    #第一个参数num_player\n",
    "    #第二个参数 batch_size\n",
    "    #第三个参数 hidden_size \n",
    "    h0 = torch.randn(2,3,6)\n",
    "    #输出结果\n",
    "    output,hn = rnn(x0,h0)#output只保存最后一层的结果 hn保存两层的最后的结果\n",
    "    print(output)\n",
    "    print(hn)\n",
    "_dm_rnn_for_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ba053-e3ff-474d-acf0-66f7c74d6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练过长 梯度为乘积  数量越多越接近为0 会导致梯度消失  同理可能会导致梯度爆炸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

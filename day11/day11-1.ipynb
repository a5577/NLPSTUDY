{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4ce64e-cf86-40e7-8182-8ef403bbc6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig,AutoModel,AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification,AutoModelForMaskedLM,AutoModelForQuestionAnswering\n",
    "from transformers import AutoModelForSeq2SeqLM,AutoModelForTokenClassification\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52539144-5248-4bf0-916c-538f61073a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msgs_tensor---> {'input_ids': tensor([[ 101,  872, 3221, 6443,  102,  782, 4495, 6421, 1963,  862, 6629, 1928,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])}\n",
      "不带模型头输出output---> BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.7001,  0.4651,  0.2427,  ...,  0.5753, -0.4330,  0.1878],\n",
      "         [ 0.4017,  0.1123,  0.4482,  ..., -0.2614, -0.2649, -0.1497],\n",
      "         [ 1.2000, -0.4859,  1.1970,  ...,  0.7543, -0.2404, -0.2627],\n",
      "         ...,\n",
      "         [ 0.2074,  0.4022, -0.0448,  ..., -0.0849, -0.0766, -0.2134],\n",
      "         [ 0.0879,  0.2482, -0.2356,  ...,  0.2967, -0.2357, -0.5138],\n",
      "         [ 0.4944,  0.1340, -0.2387,  ...,  0.2375, -0.1011, -0.3314]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9996,  1.0000,  0.9995,  0.9412,  0.8629,  0.9592, -0.8144, -0.9654,\n",
      "          0.9892, -0.9997,  1.0000,  0.9998, -0.1187, -0.9373,  0.9999, -1.0000,\n",
      "         -0.4253,  0.9975,  0.9942, -0.2609,  0.9998, -1.0000, -0.8675, -0.4223,\n",
      "          0.3741,  0.9994,  0.9915, -0.9450, -0.9999,  0.9996,  0.9881,  0.9999,\n",
      "          0.9333, -1.0000, -0.9998,  0.9013, -0.5617,  0.9929, -0.3016, -0.9132,\n",
      "         -0.9749, -0.9406, -0.1077, -0.9992, -0.9945,  0.7893, -1.0000, -0.9999,\n",
      "          0.1272,  0.9999, -0.9754, -1.0000,  0.6994, -0.6589,  0.5041,  0.9953,\n",
      "         -0.9998,  0.9965,  1.0000,  0.9718,  0.9997, -0.9932, -0.2060, -0.9999,\n",
      "          1.0000, -0.9994, -0.9966,  0.6295,  0.9999,  1.0000, -0.8982,  0.9972,\n",
      "          1.0000, -0.1602,  0.8893,  0.9997, -0.9996,  0.7655, -1.0000,  0.1799,\n",
      "          1.0000,  0.9990, -0.9768,  0.9102, -0.9971, -1.0000, -0.9996,  1.0000,\n",
      "         -0.6599,  0.9996,  0.9993, -0.9996, -1.0000,  0.9991, -0.9997, -0.9996,\n",
      "         -0.9822,  0.9983, -0.1918, -0.8101, -0.7915,  0.9380, -0.9995, -0.9989,\n",
      "          0.9823,  0.9987,  0.7407, -0.9998,  0.9999,  0.0801, -1.0000, -0.8879,\n",
      "         -1.0000, -0.7263, -0.9961,  0.9997,  0.1207, -0.7102,  0.9997, -0.9994,\n",
      "          0.9376, -0.9986, -0.8416, -0.4277,  0.9958,  1.0000,  0.9992, -0.9988,\n",
      "          0.9997,  1.0000,  0.9964,  0.9979, -0.9998,  0.9831,  0.9385, -0.9943,\n",
      "          0.6209, -0.8281,  1.0000,  0.9941,  0.9975, -0.9854,  0.9993, -0.9992,\n",
      "          1.0000, -1.0000,  0.9998, -1.0000, -0.9921,  0.9990,  0.9910,  1.0000,\n",
      "         -0.9472,  1.0000, -0.9862, -0.9998,  0.9996,  0.7316,  0.9984, -1.0000,\n",
      "          0.9252, -0.3892, -0.4231, -0.9009, -1.0000,  1.0000, -0.8142,  1.0000,\n",
      "          0.9946, -0.9945, -0.9557, -0.9997,  0.9114, -0.9999, -0.9816,  0.9871,\n",
      "          0.3713,  0.9994, -0.9587, -0.9868,  0.8934, -0.7461, -1.0000,  0.9932,\n",
      "         -0.5626,  0.9529,  0.8035,  0.2448,  0.9440,  0.8595, -0.6558,  1.0000,\n",
      "          0.0584,  0.9977,  0.9997,  0.1184, -0.8943, -0.9939, -1.0000, -0.7587,\n",
      "          1.0000, -0.9324, -0.9999,  0.9437, -1.0000,  0.8426, -0.7457, -0.1889,\n",
      "         -0.9885, -1.0000,  1.0000, -0.9862, -0.9996,  0.9413, -0.7013, -0.0440,\n",
      "         -0.9990,  0.6917,  0.9914, -0.3586,  0.9869, -0.9698, -0.9998,  0.9991,\n",
      "         -0.9879,  0.6996,  0.8738,  1.0000,  0.9942, -0.5514, -0.2188,  1.0000,\n",
      "          0.7806, -1.0000,  0.9700, -0.9952, -0.6657,  1.0000, -0.9977,  0.8551,\n",
      "          1.0000,  0.9636,  1.0000, -0.7059, -0.9997, -0.9995,  1.0000,  0.9720,\n",
      "          1.0000, -0.9999, -0.9971,  0.4543, -0.9393, -1.0000, -0.9992, -0.4437,\n",
      "          0.9993,  1.0000,  0.2649, -0.9964, -0.7573, -0.9994,  1.0000, -0.9884,\n",
      "          1.0000,  0.9977, -0.9990, -0.9880,  0.4585, -0.8447, -0.9996,  0.8331,\n",
      "         -0.9999, -0.9958, -1.0000,  0.9627, -0.9998, -1.0000,  0.9440,  1.0000,\n",
      "          0.8333, -1.0000,  0.9999,  0.9989, -0.5691, -0.9913,  0.8767, -1.0000,\n",
      "          1.0000, -0.9998,  0.9679, -0.7449, -0.9967,  0.9371,  0.9996,  0.9969,\n",
      "         -0.9987,  0.2781, -0.9930, -0.9768, -0.5785,  0.9273, -0.7460,  0.9389,\n",
      "         -0.9569, -0.9309,  0.4148, -0.9587, -0.9999,  0.9023,  1.0000, -0.9883,\n",
      "          1.0000,  0.9609,  1.0000,  0.8448, -0.9994,  0.9976,  0.5122,  0.0215,\n",
      "         -0.6921, -0.9911,  0.9692,  0.7839, -0.9629, -0.9993,  1.0000,  0.9986,\n",
      "          0.9722,  0.9191, -0.8145,  0.0523,  0.9769, -0.9983,  0.9967, -0.9999,\n",
      "         -0.9724,  0.9997,  1.0000,  0.9990,  0.5845, -0.9095,  0.9991, -0.9995,\n",
      "          0.9999, -0.9998,  0.9994, -0.9827,  0.8794, -0.9829, -0.9937,  1.0000,\n",
      "          0.9931, -0.8369,  0.9990, -0.9921,  0.9967,  0.9857,  0.9952,  0.9985,\n",
      "          0.9940,  1.0000, -0.9991, -0.9955, -0.9785, -0.9975, -0.9992, -1.0000,\n",
      "          0.4263, -0.9989, -0.9958,  0.5627,  0.8774,  0.9492, -0.9880,  0.1504,\n",
      "         -0.2917,  0.6286, -0.8083,  0.2261,  0.7591, -0.9982, -0.9979, -1.0000,\n",
      "         -0.9998,  0.7946,  0.9998, -0.9999,  0.9997, -1.0000, -0.8349,  0.9447,\n",
      "         -0.7435, -0.9728,  1.0000, -1.0000,  0.9617,  0.9998,  1.0000,  0.9997,\n",
      "          0.9999, -0.9246, -1.0000, -0.9990, -1.0000, -1.0000, -0.9999, -0.2468,\n",
      "         -0.7244, -1.0000, -0.9231,  0.9859,  1.0000,  0.9955, -0.9999, -0.2388,\n",
      "         -0.9996, -0.9948,  0.9993, -0.9915, -0.9998,  0.9846, -0.1995,  1.0000,\n",
      "         -0.9179,  0.8934,  0.9633,  0.3846,  0.9978, -1.0000,  0.9777,  1.0000,\n",
      "          0.8270, -1.0000, -0.9940, -0.8283, -1.0000, -0.4816,  0.8760,  0.9999,\n",
      "         -0.9999, -0.8923, -0.9936,  0.9548,  0.9979,  0.9999,  0.9997,  0.9235,\n",
      "          0.9406,  0.9983,  0.7295,  1.0000,  0.6667, -0.9994,  0.9999, -0.4945,\n",
      "          0.5540, -0.9999,  0.9991,  0.7122,  1.0000,  0.9665, -0.3654, -0.9461,\n",
      "         -0.9902,  0.9966,  1.0000, -0.9961, -0.8890, -0.9995, -0.9999, -0.9987,\n",
      "         -0.9508, -0.2642, -0.9984, -0.9997,  0.6098,  0.8885,  1.0000,  1.0000,\n",
      "          0.9981, -0.9100, -0.9480,  0.9912, -0.7464,  0.9769, -0.9499, -1.0000,\n",
      "         -0.9971, -0.9992,  0.9999, -0.4936, -0.8480, -0.9288,  0.6369,  0.9595,\n",
      "         -1.0000, -0.9862, -0.9889,  0.8918,  1.0000, -0.9997,  0.9993, -0.9998,\n",
      "          0.8408,  0.7151,  0.9403,  0.9850, -0.4701,  0.3719, -0.6640,  0.8159,\n",
      "          0.9750,  0.9990, -0.9798, -0.4879,  0.9999, -0.9430,  0.9999,  0.1569,\n",
      "          0.7227,  0.9608,  1.0000,  0.9370,  0.9981,  0.9488,  1.0000,  1.0000,\n",
      "         -0.9945,  0.8866,  0.4383, -0.9918,  0.1478,  0.3349,  1.0000,  0.4956,\n",
      "         -0.9814, -0.9998,  0.9932,  0.9998,  1.0000, -0.4677,  0.9984, -0.6361,\n",
      "          0.9627,  0.6988,  0.7929,  0.5375,  0.5895,  0.9987,  0.9998, -1.0000,\n",
      "         -1.0000, -1.0000,  1.0000,  0.9998, -0.9156, -1.0000,  0.9998, -0.8772,\n",
      "          0.9853,  0.9944,  0.4373, -0.9589,  0.6754, -0.9999,  0.0689, -0.0256,\n",
      "          0.9742,  0.6449,  0.9997, -0.9999,  0.4106,  1.0000, -0.4217,  1.0000,\n",
      "          0.2841, -0.9997,  0.9997, -0.9964, -1.0000, -0.8751,  0.9999,  0.9993,\n",
      "         -0.1483, -0.1713,  1.0000, -0.9998,  1.0000, -1.0000,  0.5630, -0.9996,\n",
      "          1.0000, -0.9953, -0.9835, -0.9605,  0.9920,  0.9835, -0.8489,  1.0000,\n",
      "          0.1431, -0.9286,  0.7053, -0.9962, -0.9916, -0.9933, -0.3910, -1.0000,\n",
      "          0.9486,  0.4874, -0.7979, -0.9879, -1.0000,  1.0000, -0.8523, -0.9817,\n",
      "          1.0000, -0.9975, -1.0000,  0.9848, -0.9997,  0.6230,  0.9949,  0.6910,\n",
      "          0.5688, -1.0000,  0.5933,  1.0000, -0.9994, -0.8590, -0.9709, -0.9712,\n",
      "          0.9897,  0.9994,  0.9288, -0.8009,  0.9769,  0.9320,  0.9201, -0.3857,\n",
      "          0.4657, -0.9996, -0.9866, -0.9884, -0.9990, -0.9998, -1.0000,  1.0000,\n",
      "          0.9999,  1.0000, -0.8597, -0.8931,  0.9556,  0.9916, -0.9994, -0.3621,\n",
      "          0.8778,  0.9885, -0.0600, -0.9998, -0.6338, -1.0000, -0.6789,  0.5136,\n",
      "         -0.8639,  0.7343,  1.0000,  1.0000, -0.9997, -0.9996, -0.9991, -0.9993,\n",
      "          1.0000,  0.9996,  0.9999, -0.9138, -0.7661,  0.9986, -0.8770, -0.5765,\n",
      "         -0.9587, -0.9981, -1.0000,  0.9713, -0.9767, -0.9999,  0.9969,  1.0000,\n",
      "          0.5653, -1.0000, -0.8909,  0.9998,  0.9993,  1.0000,  0.2735,  0.9999,\n",
      "         -0.9979,  0.9917, -0.9997,  1.0000, -1.0000,  1.0000,  1.0000,  0.9995,\n",
      "          0.9964, -0.9971,  0.9764, -0.9959, -0.0842,  0.9948, -0.7066, -0.9992,\n",
      "          0.4281,  0.9957, -0.9865,  1.0000,  0.6095,  0.0991,  0.9454,  0.5479,\n",
      "          0.9871, -0.3591, -0.9998,  0.9965,  0.9977,  0.9903,  1.0000,  0.9478,\n",
      "          1.0000, -0.9378, -0.9801,  0.9947, -0.7748,  0.8312, -1.0000,  1.0000,\n",
      "          1.0000, -0.9999, -0.9994,  0.5908,  0.8456,  1.0000,  0.9974,  0.9919,\n",
      "          0.9364,  0.7240,  0.9984, -0.9997,  0.8080, -0.9896, -0.9983,  1.0000,\n",
      "         -0.9934,  0.9993, -0.9982,  0.9999, -0.9989,  0.9164,  0.9946,  0.9855,\n",
      "         -0.9967,  1.0000,  0.8626, -0.9993, -0.9704, -0.9993, -0.9971,  0.8522]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "outputs.last_hidden_state.shape---> torch.Size([1, 30, 768])\n",
      "outputs.pooler_output.shape---> torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# 特征提取任务-不带任务输出头的任务\n",
    "def dm02_test_feature_extraction():\n",
    "    # 1 加载tokenizer\n",
    "    my_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=r'D:\\bert')\n",
    "\n",
    "    # 2 加载模型\n",
    "    my_model = AutoModel.from_pretrained(pretrained_model_name_or_path = r'D:\\bert')\n",
    "\n",
    "    # 3 文本转张量\n",
    "    message = ['你是谁', '人生该如何起头']\n",
    "    msgs_tensor = my_tokenizer.encode_plus(text=message, return_tensors='pt', truncation=True, pad_to_max_length=True, max_length=30)\n",
    "    print('msgs_tensor--->', msgs_tensor)\n",
    "    #token_type_ids第一个句子和第二个句子  attention_mask中1为句子 后面为补齐\n",
    "    # 4 给模型送数据提取特征\n",
    "    my_model.eval()\n",
    "    output = my_model(**msgs_tensor)#msgs_tensor为字典 传入字典可以直接用**\n",
    "    print('不带模型头输出output--->', output)\n",
    "    #最后一个隐藏层的状态\n",
    "    print('outputs.last_hidden_state.shape--->', output.last_hidden_state.shape)  # torch.Size([1, 30, 768])\n",
    "    #池化层 防止过拟合 让模型更简单\n",
    "    print('outputs.pooler_output.shape--->', output.pooler_output.shape)  # torch.Size([1, 768])\n",
    "dm02_test_feature_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f90b23d2-d90f-4d82-ab69-5884a8cddb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:\\bert-wwm were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input---> {'input_ids': tensor([[ 101, 2769, 2682, 3209, 1921, 1343,  103, 2157, 1391, 7649,  119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "output---> MaskedLMOutput(loss=None, logits=tensor([[[ -9.9017,  -9.6006,  -9.8032,  ...,  -7.9744,  -7.7402,  -8.2912],\n",
      "         [-14.3878, -15.0353, -14.7893,  ..., -10.0437, -10.5279,  -9.7544],\n",
      "         [-14.2215, -14.1145, -14.5770,  ...,  -6.3246,  -4.1784,  -4.6072],\n",
      "         ...,\n",
      "         [-14.6938, -16.8133, -15.1296,  ...,  -9.2327,  -8.1931, -15.2430],\n",
      "         [-10.8649, -11.4887, -11.5731,  ...,  -6.5378,  -0.8715,  -5.3870],\n",
      "         [-11.8495, -11.8358, -12.0314,  ...,  -8.4242,  -6.2741,  -8.2787]]],\n",
      "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "output.logits---> torch.Size([1, 12, 21128])\n",
      "1961\n",
      "打印概率最高的字: ['她']\n"
     ]
    }
   ],
   "source": [
    "# 完型填空任务\n",
    "def dm03_test_fill_mask():\n",
    "\n",
    "    # 1 加载tokenizer\n",
    "    #modename = \"chinese-bert-wwm\"\n",
    "    # modename = \"bert-base-chinese\"\n",
    "    my_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path =r'D:\\bert-wwm')\n",
    "\n",
    "    # 2 加载模型\n",
    "    my_model = AutoModelForMaskedLM.from_pretrained(pretrained_model_name_or_path =r'D:\\bert-wwm')\n",
    "    #print(my_model)\n",
    "    # 3 文本转张量\n",
    "    input = my_tokenizer.encode_plus('我想明天去[MASK]家吃饭.', return_tensors='pt')#104行为MASK\n",
    "    print('input--->', input)\n",
    "\n",
    "    # 4 给模型送数据提取特征\n",
    "    my_model.eval()\n",
    "    output = my_model(**input)\n",
    "    print('output--->', output) \n",
    "    print('output.logits--->', output.logits.shape) # [1,12,21128]需要从结果中找到mask对应位输出的21128概率值\n",
    "\n",
    "    # 5 取概率最高\n",
    "    mask_pred_idx = torch.argmax(output.logits[0][6]).item()#【6】处即为MASK对应的位置 取这个地方最大的向量值\n",
    "    print(mask_pred_idx)\n",
    "    print('打印概率最高的字:', my_tokenizer.convert_ids_to_tokens([mask_pred_idx]))\n",
    "dm03_test_fill_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba673bd6-9efc-47e4-9f54-c12fb222bcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:\\roberta were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input---> {'input_ids': tensor([[ 101, 2769, 3221, 6443, 8043,  102, 2769, 1373, 2476,  676, 2769, 3221,\n",
      "          671,  702, 4923, 2415, 1447, 2769, 4638, 1599, 1962, 3221, 2802, 5074,\n",
      "         4413,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}\n",
      "output---> QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ -1.9978, -11.4788, -12.6324, -11.8324, -12.4148, -11.9371,  -2.7246,\n",
      "          -6.6402,   3.9131,  -2.9533,  -7.0866,  -9.5696,  -4.2775,  -8.9042,\n",
      "           0.5753,  -6.9468,  -7.0469,  -8.5334, -11.3796,  -9.3905, -11.0242,\n",
      "         -11.1047,  -5.7124,  -2.7293,  -7.5896, -12.6013]],\n",
      "       grad_fn=<CloneBackward0>), end_logits=tensor([[ -1.3483, -12.0141, -11.6312, -11.6629, -11.9607, -12.0039,  -4.6118,\n",
      "          -7.4034,  -2.3499,   4.7159,  -7.2880,  -9.5317,  -6.6742,  -6.0915,\n",
      "          -7.0023,  -4.9691,   1.4515,  -7.8329,  -9.0895, -10.3742,  -8.7482,\n",
      "          -9.8567,  -7.2930,  -5.8163,  -1.7323, -12.2525]],\n",
      "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n",
      "start,end tensor(8) tensor(10)\n",
      "question: 我是谁？ answer: 张三\n",
      "input---> {'input_ids': tensor([[ 101, 2769, 3221,  976,  784,  720, 4638, 8043,  102, 2769, 1373, 2476,\n",
      "          676, 2769, 3221,  671,  702, 4923, 2415, 1447, 2769, 4638, 1599, 1962,\n",
      "         3221, 2802, 5074, 4413,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n",
      "output---> QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ -2.2018, -12.1859, -12.2946, -12.1527, -11.8349, -12.2244, -12.5048,\n",
      "         -12.3524, -11.7760,  -5.8289,  -9.2119,  -2.6119,  -5.2339,  -8.9569,\n",
      "          -9.6797,  -3.3958,  -8.2407,   4.9940,  -4.8070,  -4.2607, -10.8480,\n",
      "         -12.1394, -10.7349, -12.1574, -12.8387,  -7.0354,  -3.5467,  -7.8111,\n",
      "         -13.3356]], grad_fn=<CloneBackward0>), end_logits=tensor([[ -1.6445, -12.8343, -12.8161, -12.5158, -12.6665, -12.5056, -12.3809,\n",
      "         -12.7396, -12.8024,  -7.9332,  -8.8340,  -6.6529,  -1.3183,  -8.8469,\n",
      "         -10.3757,  -7.1896,  -6.2773,  -4.6138,   1.4756,   5.7726,  -9.4083,\n",
      "         -10.5839, -10.6289,  -9.3858, -10.9594,  -8.2169,  -5.4891,  -2.3510,\n",
      "         -11.5964]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n",
      "start,end tensor(17) tensor(20)\n",
      "question: 我是做什么的？ answer: 程序员\n",
      "input---> {'input_ids': tensor([[ 101, 2769, 4638, 4263, 1962, 3221,  784,  720, 8043,  102, 2769, 1373,\n",
      "         2476,  676, 2769, 3221,  671,  702, 4923, 2415, 1447, 2769, 4638, 1599,\n",
      "         1962, 3221, 2802, 5074, 4413,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n",
      "output---> QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ -2.3626, -11.8109, -12.0014, -11.8726, -12.1884, -11.8566,  -9.7494,\n",
      "         -11.1390, -11.7340, -11.1959,  -7.9703, -10.8244,  -6.8488,  -7.3804,\n",
      "         -10.2713, -11.6897,  -9.4933, -10.8606,  -5.7595,  -9.2127, -10.6542,\n",
      "          -9.7533, -10.6992,  -7.8148, -10.3834,  -9.6321,   3.5385,   1.6568,\n",
      "          -5.6116, -13.1424]], grad_fn=<CloneBackward0>), end_logits=tensor([[ -1.6990, -11.7872, -11.4344, -11.7961, -10.9140, -11.3832, -10.8778,\n",
      "         -10.4030, -11.2210, -10.8597,  -9.3072,  -8.4402,  -9.3098,  -4.2836,\n",
      "         -10.4882, -10.9745,  -9.0912,  -8.4680,  -8.4498,  -7.5450,  -6.3684,\n",
      "         -10.0657,  -9.6511,  -9.8094,  -7.6295, -10.5727,  -2.3552,  -2.5650,\n",
      "           4.3905, -10.5694]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n",
      "start,end tensor(26) tensor(29)\n",
      "question: 我的爱好是什么？ answer: 打篮球\n"
     ]
    }
   ],
   "source": [
    "def dm04_test_question_answering():\n",
    "\n",
    "    # 1 加载tokenizer\n",
    "    my_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path =r'D:\\roberta')\n",
    "\n",
    "    # 2 加载模型\n",
    "    my_model = AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name_or_path =r'D:\\roberta')\n",
    "\n",
    "    # 3 文本转张量\n",
    "    # 文字中的标点符号如果是中文的话，会影响到预测结果 也可以去掉标点符号\n",
    "    context = '我叫张三 我是一个程序员 我的喜好是打篮球'\n",
    "    questions = ['我是谁？', '我是做什么的？', '我的爱好是什么？']\n",
    "\n",
    "    # 4 给模型送数据 模型做抽取式问答 一个问题一个问题去解答\n",
    "    my_model.eval()\n",
    "    for question in questions:\n",
    "        input = my_tokenizer.encode_plus(question, context, return_tensors='pt')#前面为问题后面为句子\n",
    "        print('input--->', input)\n",
    "        output = my_model(**input)\n",
    "        print('output--->', output)#从start和end中找到句子起始和结束构成答案\n",
    "        start, end = torch.argmax(output.start_logits), torch.argmax(output.end_logits) +1\n",
    "        print('start,end',start,end)\n",
    "        answer =''.join(my_tokenizer.convert_ids_to_tokens(input['input_ids'][0][start:end] ))#找到对应索引\n",
    "        print('question:', question, 'answer:', answer)\n",
    "dm04_test_question_answering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88e4e14d-1383-42f2-8cd6-57c8b2a74940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input---> {'input_ids': tensor([[    0, 11126,   565,    16,    10,  7891,   268,  1421, 11857, 26492,\n",
      "            15,    10,   739, 42168,     9,  2370,   414,    11,    10,  1403,\n",
      "            12, 16101, 25376,  2734,     4,   152,   839,    24,    21, 11857,\n",
      "         26492,    15,     5,  6087, 14301,   129,     6,    19,   117,  5868,\n",
      "          6348, 14139,   106,    11,   143,   169,    36,  5488,    16,   596,\n",
      "            24,    64,   304,  3739,     9,  3271,   577,   414,    43,    19,\n",
      "            41,  8408,   609,     7,  5368, 16584,     8, 14105,    31,   167,\n",
      "         14301,     4,   901, 12810,     6,    24,    21, 11857, 26492,    19,\n",
      "            80, 10366,    35, 47661,   196,  2777, 19039,    36, 10537,   448,\n",
      "          3256,   602,    10,  3645,     6,     5,  1421, 22422, 17683,   379,\n",
      "           207,     9,     5,  1617,    11,     5,  8135,   172,   422,     5,\n",
      "          1445, 24397,  3645,   149,     5,  1421,     8,    34,     7,  7006,\n",
      "             5, 24397,  1617,     4,   152,    16,   430,    31,  2065, 35583,\n",
      "         26739,  4836,    36,   500, 20057,    29,    43,    14,  2333,   192,\n",
      "             5,  1617,    65,    71,     5,    97,     6,    50,    31,  7241,\n",
      "          1688, 39292,  3092,   101,   272, 10311,    61, 16985, 11445,     5,\n",
      "           499, 22121,     4,    85,  2386,     5,  1421,     7,  1532,    10,\n",
      "          2311, 43606,   337,  8985,     9,     5,  3645,     4, 19192,  3645,\n",
      "         16782,    36,   487,  4186,  3256,     5,  3092, 10146, 26511,  1626,\n",
      "            80, 24397, 11305,    25, 16584,   148, 11857, 32155,     4,  7411,\n",
      "            51, 20719,     7, 11305,    14,    58,   220,     7,   349,    97,\n",
      "            11,     5,  1461,  2788,     6,  2128,    45,     4,    20,  1421,\n",
      "           172,    34,     7,  7006,   114,     5,    80, 11305,    58,   511,\n",
      "           349,    97,    50,    45,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "output---> tensor([[    2,     0, 11126,   565,    16,    10,  7891,   268,  1421, 11857,\n",
      "         26492,    15,    10,   739, 42168,     9,  2370,   414,    11,    10,\n",
      "          1403,    12, 16101, 25376,  2734,   479,    85,    21, 11857, 26492,\n",
      "            19,    80, 10366,    35, 31755,   196,  2777, 19039,    36, 10537,\n",
      "           448,    43,     8,   220,  3645, 16782,    36,   487,  4186,    43,\n",
      "            20,  3092, 10146, 26511,  1626,    80, 24397, 11305,    25, 16584,\n",
      "           148, 11857, 32155,   479,  7411,    51, 20719,     7, 11305,    14,\n",
      "            58,   220,     7,   349,    97,    11,     5,  1461,  2788,     6,\n",
      "          2128,    45,   479,     2]])\n",
      "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion . It was pretrained with two objectives: Masked language modeling (MLM) and next sentence prediction (NSP) The models concatenates two masked sentences as inputs during pretraining . Sometimes they correspond to sentences that were next to each other in the original text, sometimes not .\n"
     ]
    }
   ],
   "source": [
    "# 文本摘要任务\n",
    "def dm05_test_summarization():\n",
    "    text = \"BERT is a transformers model pretrained on a large corpus of English data \" \\\n",
    "           \"in a self-supervised fashion. This means it was pretrained on the raw texts \" \\\n",
    "           \"only, with no humans labelling them in any way (which is why it can use lots \" \\\n",
    "           \"of publicly available data) with an automatic process to generate inputs and \" \\\n",
    "           \"labels from those texts. More precisely, it was pretrained with two objectives:Masked \" \\\n",
    "           \"language modeling (MLM): taking a sentence, the model randomly masks 15% of the \" \\\n",
    "           \"words in the input then run the entire masked sentence through the model and has \" \\\n",
    "           \"to predict the masked words. This is different from traditional recurrent neural \" \\\n",
    "           \"networks (RNNs) that usually see the words one after the other, or from autoregressive \" \\\n",
    "           \"models like GPT which internally mask the future tokens. It allows the model to learn \" \\\n",
    "           \"a bidirectional representation of the sentence.Next sentence prediction (NSP): the models\" \\\n",
    "           \" concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to \" \\\n",
    "           \"sentences that were next to each other in the original text, sometimes not. The model then \" \\\n",
    "           \"has to predict if the two sentences were following each other or not.\"\n",
    "\n",
    "    # 1 加载tokenizer\n",
    "    my_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=r\"D:\\distibart\")\n",
    "\n",
    "    # 2 加载模型\n",
    "    my_model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path=r'D:\\distibart')\n",
    "\n",
    "    # 3 文本转张量\n",
    "    input = my_tokenizer([text], return_tensors='pt')\n",
    "    print('input--->', input)\n",
    "    my_model.eval()\n",
    "    output = my_model.generate(input.input_ids)\n",
    "    print('output--->', output)\n",
    "\n",
    "    # 5 处理摘要结果\n",
    "    # 5-1 decode 的 skip_special_tokens 参数可以去除 token 前面的特殊字符\n",
    "    print(my_tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))\n",
    "\n",
    "dm05_test_summarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee6319e5-d922-426e-8ce7-f6653e398f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs---> torch.Size([1, 17]) tensor([[ 101, 7826, 6813, 1044, 4495, 4638,  807, 6134,  868, 3221,  517, 4312,\n",
      "          782, 3189, 6381,  518,  102]])\n",
      "logits---> torch.Size([1, 17, 32])\n",
      "input_tokens---> ['[CLS]', '鲁', '迅', '先', '生', '的', '代', '表', '作', '是', '《', '狂', '人', '日', '记', '》', '[SEP]']\n",
      "[('鲁', 'B-name'), ('迅', 'I-name'), ('先', 'O'), ('生', 'O'), ('的', 'O'), ('代', 'O'), ('表', 'O'), ('作', 'O'), ('是', 'O'), ('《', 'B-book'), ('狂', 'I-book'), ('人', 'I-book'), ('日', 'I-book'), ('记', 'I-book'), ('》', 'I-book')]\n"
     ]
    }
   ],
   "source": [
    "#NER任务\n",
    "def dm06_test_ner():\n",
    "    # 1 加载tokenizer 加载模型 加载配置文件\n",
    "    # https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese\n",
    "    my_tokenizer = AutoTokenizer.from_pretrained(r'D:\\cnn')\n",
    "    my_model = AutoModelForTokenClassification.from_pretrained(r'D:\\cnn')\n",
    "    config = AutoConfig.from_pretrained(r'D:\\cnn')\n",
    "    # 2 数据张量化\n",
    "    inputs = my_tokenizer.encode_plus('鲁迅先生的代表作是《狂人日记》', return_tensors='pt')\n",
    "    print('inputs--->', inputs.input_ids.shape, inputs.input_ids) # torch.Size([1, 17])\n",
    "\n",
    "    # 3 送入模型 预测ner概率 每个字预测的标签概率\n",
    "    my_model.eval()\n",
    "    logits = my_model(inputs.input_ids).logits\n",
    "    print('logits--->', logits.shape)           # torch.Size([1, 17, 32])\n",
    "\n",
    "    # 4 对预测数据 进行显示\n",
    "    input_tokens = my_tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "    print('input_tokens--->', input_tokens)\n",
    "    outputs = []\n",
    "\n",
    "    for token, value in zip(input_tokens, logits[0]):\n",
    "\n",
    "        if token in my_tokenizer.all_special_tokens:#去除[CLS][SEP]\n",
    "            continue\n",
    "\n",
    "        # 获得每个字预测概率最大的标签索引\n",
    "        idx = torch.argmax(value).item()\n",
    "\n",
    "        # 打印索引对应标签\n",
    "        outputs.append((token, config.id2label[idx]))\n",
    "\n",
    "    print(outputs)\n",
    "dm06_test_ner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13969a30-b855-4f5c-a6eb-60dd14e2e65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\.conda\\envs\\DL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 1.12.0+cu113\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae08ddbf-245c-412b-aae2-eccd3b7de8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:\\bert-wwm were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input---> {'input_ids': tensor([[ 101, 2769, 2682, 3209, 1921, 1343,  103, 2157, 1391, 7649,  119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "output---> MaskedLMOutput(loss=None, logits=tensor([[[ -9.9017,  -9.6006,  -9.8032,  ...,  -7.9744,  -7.7402,  -8.2912],\n",
      "         [-14.3878, -15.0353, -14.7893,  ..., -10.0437, -10.5279,  -9.7544],\n",
      "         [-14.2215, -14.1145, -14.5770,  ...,  -6.3246,  -4.1784,  -4.6072],\n",
      "         ...,\n",
      "         [-14.6938, -16.8133, -15.1296,  ...,  -9.2327,  -8.1931, -15.2430],\n",
      "         [-10.8649, -11.4887, -11.5731,  ...,  -6.5378,  -0.8715,  -5.3870],\n",
      "         [-11.8495, -11.8358, -12.0314,  ...,  -8.4242,  -6.2741,  -8.2787]]],\n",
      "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "output.logits---> torch.Size([1, 12, 21128])\n",
      "1961\n",
      "打印概率最高的字: ['她']\n"
     ]
    }
   ],
   "source": [
    "#具体模型方式完成完形填空任务\n",
    "# 完型填空任务\n",
    "def dm03_test_fill_mask():\n",
    "\n",
    "    # 1 加载tokenizer\n",
    "    #modename = \"chinese-bert-wwm\"\n",
    "    # modename = \"bert-base-chinese\"\n",
    "    my_tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path =r'D:\\bert-wwm')\n",
    "\n",
    "    # 2 加载模型\n",
    "    my_model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path =r'D:\\bert-wwm')#和自动模型区别只在于导包不相同\n",
    "    #print(my_model)\n",
    "    # 3 文本转张量\n",
    "    input = my_tokenizer.encode_plus('我想明天去[MASK]家吃饭.', return_tensors='pt')#104行为MASK\n",
    "    print('input--->', input)\n",
    "\n",
    "    # 4 给模型送数据提取特征\n",
    "    my_model.eval()\n",
    "    output = my_model(**input)\n",
    "    print('output--->', output) \n",
    "    print('output.logits--->', output.logits.shape) # [1,12,21128]需要从结果中找到mask对应位输出的21128概率值\n",
    "\n",
    "    # 5 取概率最高\n",
    "    mask_pred_idx = torch.argmax(output.logits[0][6]).item()#【6】处即为MASK对应的位置 取这个地方最大的向量值\n",
    "    print(mask_pred_idx)\n",
    "    print('打印概率最高的字:', my_tokenizer.convert_ids_to_tokens([mask_pred_idx]))\n",
    "dm03_test_fill_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003fe58-c133-4e0c-884b-03b93bc2e0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
